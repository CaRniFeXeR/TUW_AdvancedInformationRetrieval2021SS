{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"D8usSW9Bwv4h"},"source":["# AIR - Exercise in Google Colab\n","\n","## Colab Preparation\n","\n","Open via google drive -> right click: open with Colab\n","\n","**Get a GPU**\n","\n","Toolbar -> Runtime -> Change Runtime Type -> GPU\n","\n","**Mount Google Drive**\n","\n","* Download data and clone your github repo to your Google Drive folder\n","* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n","* Commit to Github locally from the synced drive\n","\n","**Keep Alive**\n","\n","When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n","\n","**Get Started**\n","\n","Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Sfiw_6jZ0uWa"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -r ../requirements.txt"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"IUVVDw1m2sed"},"outputs":[],"source":["import torch\n","\n","print(\"Version:\",torch.__version__)\n","print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n","print(\"Random tensor:\",torch.rand(10,device=\"cuda\")) # check that pytorch works "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fvQMmxs0x_x8"},"source":["# Main.py Replacement\n","\n","-> add your code here\n","\n","- Replace *air_test* with your google drive location in the sys.path.append()"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Y_IEUP_2-099"},"outputs":[],"source":["from allennlp.common import Params, Tqdm\n","from allennlp.common.util import prepare_environment\n","from allennlp.data.dataloader import PyTorchDataLoader\n","prepare_environment(Params({})) # sets the seeds to be fixed\n","\n","import torch\n","\n","from allennlp.data.vocabulary import Vocabulary\n","\n","from allennlp.modules.token_embedders import Embedding\n","from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n","\n","from data_loading import *\n","from model_knrm import *\n","from model_conv_knrm import *\n","from model_tk import *\n","\n","# change paths to your data directory\n","config = {\n","    \"vocab_directory\": \"../data/allen_vocab_lower_10\",\n","    \"pre_trained_embedding\": \"../data/glove.42B.300d.txt\",\n","    \"model\": \"knrm\",\n","    \"train_data\": \"../data/triples.train.tsv\",\n","    \"validation_data\": \"../data/tuples.validation.tsv\",\n","    \"test_data\":\"../data/tuples.test.tsv\",\n","}\n","\n","#\n","# data loading\n","#\n","\n","vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n","tokens_embedder = Embedding(vocab=vocab,\n","                           pretrained_file= config[\"pre_trained_embedding\"],\n","                           embedding_dim=300,\n","                           trainable=True,\n","                           padding_index=0)\n","word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n","\n","# recommended default params for the models (but you may change them if you want)\n","if config[\"model\"] == \"knrm\":\n","    model = KNRM(word_embedder, n_kernels=11)\n","elif config[\"model\"] == \"conv_knrm\":\n","    model = Conv_KNRM(word_embedder, n_grams=3, n_kernels=11, conv_out_dim=128)\n","elif config[\"model\"] == \"tk\":\n","    model = TK(word_embedder, n_kernels=11, n_layers = 2, n_tf_dim = 300, n_tf_heads = 10)\n","\n","\n","# todo optimizer, loss \n","\n","print('Model',config[\"model\"],'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n","print('Network:', model)\n","\n","#\n","# train\n","#\n","\n","_triple_reader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n","_triple_reader = _triple_reader.read(config[\"train_data\"])\n","_triple_reader.index_with(vocab)\n","loader = PyTorchDataLoader(_triple_reader, batch_size=32)\n","\n","for epoch in range(2):\n","\n","    for batch in Tqdm.tqdm(loader):\n","        # todo train loop\n","        pass\n","\n","\n","#\n","# eval (duplicate for validation inside train loop - but rename \"loader\", since\n","# otherwise it will overwrite the original train iterator, which is instantiated outside the loop)\n","#\n","\n","_tuple_reader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30)\n","_tuple_reader = _tuple_reader.read(config[\"test_data\"])\n","_tuple_reader.index_with(vocab)\n","loader = PyTorchDataLoader(_tuple_reader, batch_size=128)\n","\n","for batch in Tqdm.tqdm(loader):\n","    # todo test loop \n","    # todo evaluation\n","    pass\n"]}]}